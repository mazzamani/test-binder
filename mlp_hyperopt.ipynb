{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_hyperopt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2umSnto2_Fj",
        "colab_type": "text"
      },
      "source": [
        "Explain here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxBKMV3Y8mFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install hpbandster"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHGvAib5YvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(100)\n",
        "\n",
        "# for pytorch\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import NamedTuple\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "# for hyperparameter optimization\n",
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "from hpbandster.core.worker import Worker\n",
        "import hpbandster.core.nameserver as hpns\n",
        "import hpbandster.core.result as hpres\n",
        "from hpbandster.optimizers import BOHB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PifpfwN25cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def twospirals(n_points, difficulty, noise=1.):\n",
        "    \"\"\"\n",
        "     Returns the two spirals dataset.\n",
        "    \"\"\"\n",
        "    n = np.sqrt(np.random.rand(n_points,1)) * difficulty * (2*np.pi)/360\n",
        "    d1x = -np.cos(n)*n + np.random.rand(n_points,1) * noise\n",
        "    d1y = np.sin(n)*n + np.random.rand(n_points,1) * noise\n",
        "    return (np.vstack((np.hstack((d1x,d1y)),np.hstack((-d1x,-d1y)))), \n",
        "            np.hstack((np.zeros(n_points),np.ones(n_points))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfhvqEj1Rymr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SprialDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    Loading the dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, n_points, difficulty):\n",
        "        super(SprialDataSet, self).__init__()\n",
        "        X, Y = twospirals(n_points, difficulty)\n",
        "        self.set_len = len(X)\n",
        "        X = torch.from_numpy(X)\n",
        "        self.input = X.type(torch.float32)\n",
        "        \n",
        "        Y = torch.from_numpy(Y)\n",
        "        Y = Y.unsqueeze(1)\n",
        "        self.label = Y.type(torch.float32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.set_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input[idx], self.label[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgXNN13CRJ0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(MLP, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=args.dropout)\n",
        "        self.args = args\n",
        "\n",
        "        self.fc1 = nn.Linear(2, args.fc1)\n",
        "        out = args.fc1\n",
        "\n",
        "        if args.num_layers >= 2:\n",
        "            self.fc2 = nn.Linear(args.fc1, args.fc2)\n",
        "            out = args.fc2\n",
        "\n",
        "        elif args.num_layers == 3:\n",
        "            self.fc3 = nn.Linear(args.fc2, args.fc3)\n",
        "            out = args.fc3\n",
        "\n",
        "        self.fc_out = nn.Linear(out, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        \n",
        "        if self.args.num_layers >= 2:\n",
        "          x = self.dropout(torch.relu(self.fc2(x)))\n",
        "\n",
        "        elif self.args.num_layers >= 3:\n",
        "          x = self.dropout(torch.relu(self.fc3(x)))\n",
        "        \n",
        "        out = self.fc_out(x)\n",
        "        # TODO5 construct the network here\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V4NCOl0XBKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "class Optimization():\n",
        "    def __init__(self, args, loss,  train_loader, val_loader, test_loader):\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.test_loader = test_loader\n",
        "        device = args.device\n",
        "\n",
        "        self.model = MLP(args).to(device)\n",
        "        \n",
        "        print(\"number of trainable parameter = \", count_parameters(self.model))\n",
        "        \n",
        "        if args.optimizer == 'Adam':\n",
        "            self.optimizer = optim.Adam(self.model.parameters(), lr=args.rate)\n",
        "        elif args.optimizer == 'SGD':\n",
        "            self.optimizer = torch.optim.SGD(self.model.parameters(), lr=args.rate, momentum=args.sgd_momentum)\n",
        "\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=args.lr_decay_step)\n",
        "\n",
        "        self.loss = loss\n",
        "        self.device = device\n",
        "\n",
        "    def train(self):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.train()\n",
        "        for iter, data in enumerate(self.train_loader):\n",
        "            \n",
        "            inputs, labels = data \n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value\n",
        "\n",
        "    def val_eval(self):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.eval()\n",
        "        for iter, data in enumerate(self.val_loader):\n",
        "            inputs, labels = data\n",
        "            \n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            \n",
        "            # for evaluating the network, we disable the gradient calculation with the no_grad function\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss(outputs, labels)\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value\n",
        "\n",
        "    def test_eval(self, graph=False):\n",
        "        batch_counter = 0.0\n",
        "        total_loss = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for iter, data in enumerate(self.test_loader):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "           \n",
        "            # for evaluating the network, we disable the gradient calculation with the no_grad function\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = torch.mean((torch.sign(labels - 0.5) * torch.sign(outputs) > 0).type(torch.float32).to(self.device))\n",
        "\n",
        "            batch_counter += 1\n",
        "            total_loss += loss\n",
        "            \n",
        "           \n",
        "        loss_value = total_loss / batch_counter\n",
        "        return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWAwkGzSUuSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(args, train_loader, val_loader, test_loader):\n",
        "    device = torch.device(args.device)\n",
        "    best_val_error = np.inf\n",
        "\n",
        "    if args.loss == 'BCE':\n",
        "        loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    elif args.loss == 'NLL':\n",
        "        loss_function = nn.NLLLoss(reduction='mean')\n",
        "\n",
        "    optimization = Optimization(args, loss_function, train_loader, val_loader, test_loader)\n",
        "\n",
        "    train_loss_records = []\n",
        "    val_loss_records = []\n",
        "    test_loss_records = []\n",
        "\n",
        "    print(\"loading training, val and test set completed!\")\n",
        "    mistake_counter = 0  # mistakes counter for validation loss\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        train_loss = optimization.train()\n",
        "        train_loss_records.append(train_loss)\n",
        "        optimization.scheduler.step()\n",
        "\n",
        "        val_loss = optimization.val_eval()\n",
        "        val_loss_records.append(val_loss)\n",
        "\n",
        "        test_loss = optimization.test_eval()\n",
        "        test_loss_records.append(test_loss)\n",
        "\n",
        "        if epoch > 1:\n",
        "            if val_loss_records[-1] > val_loss_records[-2]:\n",
        "                mistake_counter += 1\n",
        "\n",
        "        if val_loss < best_val_error:\n",
        "            best_results = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': copy.deepcopy(optimization.model.state_dict()),\n",
        "                'model': optimization.model,\n",
        "                'best_val_error': val_loss,\n",
        "                'best_test_error': test_loss,\n",
        "                'optimizer': copy.deepcopy(optimization.optimizer),\n",
        "                'args': args\n",
        "            }\n",
        "            best_val_error = val_loss\n",
        "        print(\n",
        "            '[Epoch: %3d/%3d] LR: %0.8f  Train loss: %.4f,  Val loss: %.4f,  Test Acc: %.4f'\n",
        "            % (epoch + 1, args.epochs, optimization.scheduler.get_lr()[0], train_loss_records[epoch], val_loss_records[epoch],\n",
        "               test_loss_records[epoch]))\n",
        "        \n",
        "        if mistake_counter >= args.tol or epoch == args.epochs - 1:\n",
        "            print('Training is terminated. final epoch or validation loss has increased')\n",
        "            break\n",
        "    return test_loss, val_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqwyYF5ajNqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args(NamedTuple):\n",
        "    rate: float  # learning rate\n",
        "    lr_decay_step: int  # learning rate decay\n",
        "    batch_size: int  # minibatch size\n",
        "    epochs: int  # maximum training epochs\n",
        "    tol: int  # tolerance for the validation error increment\n",
        "    device: str  # cuda or cpu\n",
        "\n",
        "    loss: str  # loss function     \n",
        "    optimizer: str # optimizer method\n",
        "    sgd_momentum: float #\n",
        "\n",
        "    dropout: float  # the probability for dropout \n",
        "    fc1: int # 1st hidden layer's units\n",
        "    fc2: int\n",
        "    fc3: int\n",
        "    num_layers: int\n",
        "    # TODO1: add more layers or parameters if necessary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwkyj09W49X5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyTorchWorker(Worker):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        # initialize the arguments for loading the data\n",
        "        difficulty = 1000\n",
        "        batch_size = 32\n",
        "\n",
        "        training_set = SprialDataSet(1024, difficulty)\n",
        "        val_set = SprialDataSet(64, difficulty)\n",
        "        test_set = SprialDataSet(128, difficulty)\n",
        "\n",
        "        self.train_loader = DataLoader(training_set, batch_size=batch_size, num_workers=8, shuffle=True, drop_last=True)\n",
        "        self.val_loader = DataLoader(val_set, batch_size=batch_size, num_workers=8, shuffle=False, drop_last=True)\n",
        "        self.test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=8, shuffle=False, drop_last=True)\n",
        "\n",
        "    def compute(self, config, budget, working_directory, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        testing the configuration\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Decide which hyperparameters are searched by BOHB\n",
        "        new_args = Args(\n",
        "                        rate=config['lr'],\n",
        "                        lr_decay_step=30,\n",
        "                        batch_size=32,\n",
        "                        epochs=int(budget),\n",
        "                        tol=5,\n",
        "                        loss='BCE',\n",
        "                        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "\n",
        "                        optimizer=config['optimizer'],\n",
        "                        sgd_momentum= config['sgd_momentum'] if 'sgd_momentum' in config else None,\n",
        "                        dropout= config['dropout'],\n",
        "                        fc1=config['fc1'], # \n",
        "                        fc2=config['fc2'] if 'fc2' in config else None,\n",
        "                        fc3=config['fc3'] if 'fc3' in config else None,\n",
        "                        num_layers=config['num_layers']\n",
        "                        # TODO2 add the additional parameters from Args class here\n",
        "                        )\n",
        "        \n",
        "        test_loss, val_loss = main(new_args, self.train_loader, self.val_loader, self.test_loader)\n",
        "        time.sleep(0.1)\n",
        "        return ({\n",
        "            'loss': val_loss,  # remember: HpBandSter always minimizes!\n",
        "            'info': {'test accuracy': test_loss,\n",
        "                     }\n",
        "        })\n",
        "\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_configspace():\n",
        "        \"\"\"\n",
        "            It builds the configuration space with the needed hyperparameters\n",
        "            :return: ConfigurationsSpace-Object\n",
        "            \"\"\"\n",
        "        cs = CS.ConfigurationSpace()\n",
        "\n",
        "        # TODO3: add a proper condition here\n",
        "        # Type 1 condition: float\n",
        "        lr = CSH.UniformFloatHyperparameter('lr', lower=1e-4, upper=1e-1, default_value='1e-2', log=True)\n",
        "        \n",
        "        # Type 2 condition: categorical \n",
        "        # For demonstration purposes, we add different optimizers as categorical hyperparameters.\n",
        "        # To show how to use conditional hyperparameters with ConfigSpace, we'll add the optimizers 'Adam' and 'SGD'.\n",
        "        # SGD has a different parameter 'momentum'.\n",
        "        optimizer = CSH.CategoricalHyperparameter('optimizer', ['Adam', 'SGD'])\n",
        "\n",
        "        sgd_momentum = CSH.UniformFloatHyperparameter('sgd_momentum', lower=0.0, upper=0.99, default_value=0.9, log=False)\n",
        "\n",
        "        cs.add_hyperparameters([lr, optimizer, sgd_momentum])\n",
        "        #cs.add_hyperparameters([lr])\n",
        "\n",
        "        # Type 3 condition: conditional\n",
        "        # The hyperparameter sgd_momentum will be used,if the configuration\n",
        "        # contains 'SGD' as optimizer.\n",
        "        cond = CS.EqualsCondition(sgd_momentum, optimizer, 'SGD')\n",
        "        cs.add_condition(cond)\n",
        "\n",
        "        # Type 4 condition: Integer\n",
        "        fc1 = CSH.UniformIntegerHyperparameter('fc1', lower=2, upper=20, default_value=10, log=False)\n",
        "        fc2 = CSH.UniformIntegerHyperparameter('fc2', lower=2, upper=20, default_value=10, log=False)\n",
        "        fc3 = CSH.UniformIntegerHyperparameter('fc3', lower=2, upper=20, default_value=10, log=False)\n",
        "        num_layers = CSH.UniformIntegerHyperparameter('num_layers', lower=1, upper=3, default_value=2, log=False)\n",
        "\n",
        "        cs.add_hyperparameters([fc1, fc2, fc3, num_layers])\n",
        "\n",
        "        cond_fc2 = CS.GreaterThanCondition(fc2, num_layers,1)\n",
        "        cond_fc3 = CS.GreaterThanCondition(fc3, num_layers,2)\n",
        "        cs.add_condition(cond_fc2)\n",
        "        cs.add_condition(cond_fc3)\n",
        "\n",
        "        dropout = CSH.UniformFloatHyperparameter('dropout', lower=0, upper=0.9, default_value='0', log=False)\n",
        "        cs.add_hyperparameters([dropout])\n",
        "        return cs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgj2uEAw4xOU",
        "colab_type": "code",
        "outputId": "085d1507-54f9-407b-d2cf-04c88fce0e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "source": [
        "# TO record a backup from the old hyperparamter searches\n",
        "def backup_jsons(curDir):\n",
        "    for fname in ('results', 'configs'):\n",
        "        if os.path.exists(curDir + os.path.sep + fname + '.json'):\n",
        "            counter = 1\n",
        "            while os.path.exists(curDir + os.path.sep + fname + '_' + str(counter) + '.json'):\n",
        "                counter += 1\n",
        "            os.rename(curDir + os.path.sep + fname + '.json', curDir + os.path.sep + fname + '_' + str(counter) + '.json')\n",
        "\n",
        "\n",
        "args_min_budget = 1 # Minimum number of epochs for training.\n",
        "args_max_budget = 9 # Maximum number of epochs for training.\n",
        "args_n_iterations = 4 # Number of iterations performed by the optimizer\n",
        "args_worker = False # Flag to turn this into a worker process\n",
        "args_run_id = '' # A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.\n",
        "args_nic_name = 'lo' # Which network interface to use for communication.\n",
        "args_shared_directory = '.' # A directory that is accessible for all processes, e.g. a NFS share.\n",
        "args_eta = 3 # eta\n",
        "\n",
        "# Every process has to lookup the hostname\n",
        "host = hpns.nic_name_to_host(args_nic_name)\n",
        "\n",
        "\n",
        "if args_worker:\n",
        "    import time\n",
        "    time.sleep(1)   # short artificial delay to make sure the nameserver is already running\n",
        "    w = PyTorchWorker(run_id=args_run_id, host=host, timeout=120)\n",
        "    w.load_nameserver_credentials(working_directory=args_shared_directory)\n",
        "    w.run(background=False)\n",
        "    exit(0)\n",
        "\n",
        "\n",
        "# This example shows how to log live results. This is most useful\n",
        "# for really long runs, where intermediate results could already be\n",
        "# interesting. The core.result submodule contains the functionality to\n",
        "# read the two generated files (results.json and configs.json) and\n",
        "# create a Result object.\n",
        "#backup_jsons(args_shared_directory)\n",
        "result_logger = hpres.json_result_logger(directory=args_shared_directory, overwrite=True)\n",
        "\n",
        "\n",
        "# Start a nameserver:\n",
        "NS = hpns.NameServer(run_id=args_run_id, host=host, port=0, working_directory=args_shared_directory)\n",
        "ns_host, ns_port = NS.start()\n",
        "\n",
        "# Start local worker\n",
        "w = PyTorchWorker(run_id=args_run_id, host=host, nameserver=ns_host, nameserver_port=ns_port, timeout=120)\n",
        "w.run(background=True)\n",
        "\n",
        "# Run an optimizer\n",
        "bohb = BOHB(  configspace = PyTorchWorker.get_configspace(),\n",
        "                      run_id = args_run_id,\n",
        "                      eta = args_eta,\n",
        "                      host=host,\n",
        "                      nameserver=ns_host,\n",
        "                      nameserver_port=ns_port,\n",
        "                      result_logger=result_logger,\n",
        "                      min_budget=args_min_budget, \n",
        "                      max_budget=args_max_budget,\n",
        "               )\n",
        "res = bohb.run(n_iterations=args_n_iterations)\n",
        "\n",
        "\n",
        "# store results\n",
        "with open(os.path.join(args_shared_directory, 'results.pkl'), 'wb') as fh:\n",
        "    pickle.dump(res, fh)\n",
        "\n",
        "# shutdown\n",
        "bohb.shutdown(shutdown_workers=True)\n",
        "NS.shutdown()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEBUG:hpbandster:wait_for_workers trying to get the condition\n",
            "INFO:hpbandster:DISPATCHER: started the 'discover_worker' thread\n",
            "INFO:hpbandster:DISPATCHER: started the 'job_runner' thread\n",
            "DEBUG:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x7f1c75bf6da0; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:36101>\n",
            "DEBUG:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
            "INFO:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: start listening for jobs\n",
            "INFO:hpbandster:DISPATCHER: Pyro daemon running on 127.0.0.1:43013\n",
            "DEBUG:hpbandster:DISPATCHER: Starting worker discovery\n",
            "DEBUG:hpbandster:DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
            "INFO:hpbandster:DISPATCHER: discovered new worker, hpbandster.run_.worker.e1190fe56a0f.121139762526115712\n",
            "DEBUG:hpbandster:HBMASTER: number of workers changed to 1\n",
            "DEBUG:hpbandster:Enough workers to start this run!\n",
            "INFO:hpbandster:HBMASTER: starting run at 1587561074.1166441\n",
            "DEBUG:hpbandster:adjust_queue_size: lock accquired\n",
            "DEBUG:hpbandster:DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "INFO:hpbandster:HBMASTER: adjusted queue size to (0, 1)\n",
            "DEBUG:hpbandster:DISPATCHER: Finished worker discovery\n",
            "DEBUG:hpbandster:start sampling a new configuration.\n",
            "DEBUG:hpbandster:DISPATCHER: Trying to submit another job.\n",
            "DEBUG:hpbandster:DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
            "DEBUG:hpbandster:done sampling a new configuration.\n",
            "DEBUG:hpbandster:HBMASTER: schedule new run for iteration 0\n",
            "DEBUG:hpbandster:HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
            "DEBUG:hpbandster:HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
            "DEBUG:hpbandster:DISPATCHER: trying to submit job (0, 0, 0)\n",
            "DEBUG:hpbandster:DISPATCHER: trying to notify the job_runner thread.\n",
            "DEBUG:hpbandster:HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
            "DEBUG:hpbandster:HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
            "DEBUG:hpbandster:DISPATCHER: Trying to submit another job.\n",
            "DEBUG:hpbandster:DISPATCHER: starting job (0, 0, 0) on hpbandster.run_.worker.e1190fe56a0f.121139762526115712\n",
            "DEBUG:hpbandster:DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_.worker.e1190fe56a0f.121139762526115712\n",
            "DEBUG:hpbandster:DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
            "INFO:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: start processing job (0, 0, 0)\n",
            "DEBUG:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: args: ()\n",
            "DEBUG:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: kwargs: {'config': {'dropout': 0.0667058037326223, 'fc1': 17, 'lr': 0.02622787864296632, 'num_layers': 1, 'optimizer': 'SGD', 'sgd_momentum': 0.33047370621960187}, 'budget': 1.0, 'working_directory': '.'}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of trainable parameter =  69\n",
            "loading training, val and test set completed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "DEBUG:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: done with job (0, 0, 0), trying to register it.\n",
            "INFO:hpbandster.run_.worker.e1190fe56a0f.121:WORKER: registered result for job (0, 0, 0) with dispatcher\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch:   1/  1] LR: 0.02622788  Train loss: 0.6912,  Val loss: 0.6911,  Test Acc: 0.6797\n",
            "Training is terminated. final epoch or validation loss has increased\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
